{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Degree Plots\n",
    "2. Centrality Measures\n",
    "3. Community Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary on degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the degree of nodes and putting it into the list\n",
    "\n",
    "dv = dict(network.degree())\n",
    "k = list(dv.values())\n",
    "\n",
    "# Creating mean, min, max and sd for degree\n",
    "\n",
    "degreemean = np.mean(k)\n",
    "degreemin = np.min(k)\n",
    "degreemax = np.max(k)\n",
    "degreestd = np.std(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Degree Distribution 1: Histogram (Overall, In-Degree, Out-Degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot containing 3 graphs (overall, in-degree and out-degree distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n8/x7x_0smj0lj4_vlzr9zxvmwh0000gn/T/ipykernel_3004/1801764255.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Overall (formula is very similar as the degree distribution summary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mk_overall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Gets all the degree in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mp_k_overall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_overall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_overall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculate the average to be used for poisson at graph 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating parameters for graph\n",
    "\n",
    "# Overall (formula is very similar as the degree distribution summary)\n",
    "\n",
    "k_overall = sorted([d for n, d in network.degree()], reverse=True) #Gets all the degree in a list\n",
    "p_k_overall = np.unique(k_overall, return_counts=True)\n",
    "average = sum(k_overall)/len(network.nodes) # Calculate the average to be used for poisson at graph 2\n",
    "\n",
    "## Indegree\n",
    "\n",
    "k_indegree = sorted([d for n, d in network.in_degree()], reverse=True)\n",
    "p_k_indegree = np.unique(k_indegree, return_counts=True)\n",
    "average_in = sum(k_indegree)/len(network.nodes)\n",
    "\n",
    "## Outdegree\n",
    "\n",
    "k_outdegree = sorted([d for n, d in network.out_degree()], reverse=True)\n",
    "p_k_outdegree = np.unique(k_outdegree, return_counts=True)\n",
    "average_out = sum(k_outdegree)/len(network.nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Plot (Log Scale)\n",
    "\n",
    "# Overall\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# create plot\n",
    "ax1 = fig.add_subplot(3, 1, 1)\n",
    "\n",
    "# plot data\n",
    "plt.bar(p_k_overall[0], p_k_overall[1], width=1, color=\"b\", edgecolor=\"black\")\n",
    "\n",
    "# transform the scale of axes\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# aesthetics\n",
    "plt.title(\"Degree Histogram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel\n",
    "\n",
    "\n",
    "\n",
    "## In Degree Plot\n",
    "\n",
    "# create plot\n",
    "ax2 = fig.add_subplot(3, 1, 2)\n",
    "\n",
    "# plot data\n",
    "plt.bar(p_k_indegree[0], p_k_indegree[1], width=1, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "# transform the scale of axes\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "\n",
    "# aesthetics\n",
    "plt.title(\"In-Degree Histogram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel\n",
    "\n",
    "\n",
    "\n",
    "## OutDegree Plot\n",
    "\n",
    "# create plot\n",
    "ax3 = fig.add_subplot(3, 1, 3)\n",
    "\n",
    "# plot data\n",
    "plt.bar(p_k_outdegree[0], p_k_outdegree[1], width=1, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "# transform the scale of axes\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "\n",
    "# aesthetics\n",
    "plt.title(\"Out-Degree Histogram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Degree\")\n",
    "\n",
    "\n",
    "plt.show() #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Law Graph (Overall, In-degree, Out-degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power Law Graph \n",
    "\n",
    "# %% network degree distribution\n",
    "\n",
    "### Creating the overall graph\n",
    "\n",
    "## 1.1 - Power Law (Overall)\n",
    "\n",
    "# Create n value for finding the k below\n",
    "n = len(network.nodes)\n",
    "\n",
    "# get nodal degree 'k' data as list\n",
    "k_g = sorted([d for n, d in network.degree()], reverse=True)\n",
    "\n",
    "# get 'p_k'\n",
    "# --+ point-to-point probability\n",
    "p_k = np.unique(k_g, return_counts=True)\n",
    "# --+ cumulative probability\n",
    "cp_k = np.unique(k_g, return_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# poisson distribution\n",
    "\n",
    "# Creating parameters for poisson (lam = average degree of graph)\n",
    "# Average value is created at the poisson parameter codes above\n",
    "\n",
    "poisson_dist = np.random.poisson(lam=average, size=len(network.nodes()))\n",
    "k_poisson = Counter(poisson_dist)\n",
    "n_poisson = len(poisson_dist)\n",
    "\n",
    "x_poisson = list(k_poisson.keys())\n",
    "x_poisson = pd.DataFrame(x_poisson)\n",
    "y_poisson = list(k_poisson.values())\n",
    "y_poisson = pd.DataFrame(y_poisson)\n",
    "\n",
    "\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(20, 13))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax1.scatter(x_poisson, y_poisson/n_poisson, color='red')\n",
    "\n",
    "ax1.set_ylabel(\"count\")\n",
    "ax1.set_xlabel(\"($k$)\")\n",
    "\n",
    "# add plot\n",
    "ax0 = fig.add_subplot(1, 3, 1)\n",
    "\n",
    "# point-to-point data\n",
    "# --+ plot data\n",
    "ax0.scatter(p_k[0], p_k[1]/n, marker='o', color='black', alpha=0.7)\n",
    "# --+ title\n",
    "ax0.set_title(\"Degree Distribution\")\n",
    "# --+ labels\n",
    "ax0.set_ylabel(\"$Pr(k = k_{i})$\")\n",
    "ax0.set_xlabel(\"Degree $k$\")\n",
    "ax0.set_yscale('log')\n",
    "ax0.set_xscale('log')\n",
    "\n",
    "\n",
    "# Now we repeat the whole process again with in and out-degree. Maybe a function would \n",
    "# be better...\n",
    "\n",
    "## 2.1 Power Law (In-Degree)\n",
    "\n",
    "# get nodal degree 'k' data as list\n",
    "k_g = sorted([d for n, d in network.in_degree()], reverse=True) # I used the same k_g variable, not sure if should rename\n",
    "\n",
    "# get 'p_k'\n",
    "# --+ point-to-point probability\n",
    "p_k = np.unique(k_g, return_counts=True)\n",
    "# --+ cumulative probability\n",
    "cp_k = np.unique(k_g, return_index=True)\n",
    "\n",
    "\n",
    "## 2.2 Poisson Distribution\n",
    "\n",
    "# poisson distribution\n",
    "# Creating parameters for poisson (lam = average degree of graph)\n",
    "\n",
    "poisson_dist = np.random.poisson(lam=average_in, size=len(network.nodes()))\n",
    "k_poisson = Counter(poisson_dist)\n",
    "n_poisson = len(poisson_dist)\n",
    "\n",
    "x_poisson = list(k_poisson.keys())\n",
    "x_poisson = pd.DataFrame(x_poisson)\n",
    "y_poisson = list(k_poisson.values())\n",
    "y_poisson = pd.DataFrame(y_poisson)\n",
    "\n",
    "\n",
    "# create figure\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 2)\n",
    "ax3.scatter(x_poisson, y_poisson/n_poisson, color='red')\n",
    "\n",
    "ax3.set_ylabel(\"count\")\n",
    "ax3.set_xlabel(\"($k$)\")\n",
    "\n",
    "# add plot\n",
    "ax4 = fig.add_subplot(1, 3, 2)\n",
    "\n",
    "# point-to-point data\n",
    "# --+ plot data\n",
    "ax4.scatter(p_k[0], p_k[1]/n, marker='o', color='black', alpha=0.7)\n",
    "# --+ title\n",
    "ax4.set_title(\"In-Degree Distribution\")\n",
    "# --+ labels\n",
    "ax4.set_ylabel(\"$Pr(k = k_{i})$\")\n",
    "ax4.set_xlabel(\"Degree $k$\")\n",
    "ax4.set_yscale('log')\n",
    "ax4.set_xscale('log')\n",
    "\n",
    "\n",
    "## 3.1 Power Law (Out-Degree)\n",
    "\n",
    "# get nodal degree 'k' data as list\n",
    "k_g = sorted([d for n, d in network.out_degree()], reverse=True)\n",
    "\n",
    "# get 'p_k'\n",
    "# --+ point-to-point probability\n",
    "p_k = np.unique(k_g, return_counts=True)\n",
    "# --+ cumulative probability\n",
    "cp_k = np.unique(k_g, return_index=True)\n",
    "\n",
    "\n",
    "\n",
    "## 3.2 Poisson Distribution (Out-Degree)\n",
    "\n",
    "# Creating parameters for poisson (lam = average degree of graph)\n",
    "\n",
    "poisson_dist = np.random.poisson(lam=average_out, size=len(network.nodes()))\n",
    "k_poisson = Counter(poisson_dist)\n",
    "n_poisson = len(poisson_dist)\n",
    "\n",
    "x_poisson = list(k_poisson.keys())\n",
    "x_poisson = pd.DataFrame(x_poisson)\n",
    "y_poisson = list(k_poisson.values())\n",
    "y_poisson = pd.DataFrame(y_poisson)\n",
    "\n",
    "\n",
    "# create figure\n",
    "\n",
    "ax5 = fig.add_subplot(1, 3, 3)\n",
    "ax5.scatter(x_poisson, y_poisson/n_poisson, color='red')\n",
    "\n",
    "ax5.set_ylabel(\"count\")\n",
    "ax5.set_xlabel(\"($k$)\")\n",
    "\n",
    "# add plot\n",
    "ax6 = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "# point-to-point data\n",
    "# --+ plot data\n",
    "ax6.scatter(p_k[0], p_k[1]/n, marker='o', color='black', alpha=0.7)\n",
    "# --+ title\n",
    "ax6.set_title(\"Degree Distribution\")\n",
    "# --+ labels\n",
    "ax6.set_ylabel(\"$Pr(k = k_{i})$\")\n",
    "ax6.set_xlabel(\"Degree $k$\")\n",
    "ax6.set_yscale('log')\n",
    "ax6.set_xscale('log')\n",
    "\n",
    "\n",
    "# show plot (ax0+ax1 = normal, ax3+4 = in, ax5+6 = out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import degree_centrality\n",
    "from networkx.algorithms import betweenness_centrality\n",
    "from networkx.algorithms import eigenvector_centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Betweenness Centrality for a network\n",
    "bet_centrality = nx.betweenness_centrality(network)  # should take a few minutes\n",
    "nx.set_node_attributes(network, bet_centrality, 'betweenness centrality') \n",
    "\n",
    "# Creating the sorted_betweenness value\n",
    "sorted_betweenness = sorted(\n",
    "    bet_centrality.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# Finding the summary stats on between centrality\n",
    "bet_list = list(bet_centrality.values())\n",
    "\n",
    "betmean = np.mean(bet_list)\n",
    "betmin = np.min(bet_list)\n",
    "betmax = np.max(bet_list)\n",
    "betstd = np.std(bet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Eigenvector CEntrality for a network\n",
    "ev_centrality = nx.eigenvector_centrality(network, max_iter=200)\n",
    "nx.set_node_attributes(network, ev_centrality, 'eigenvector centrality')\n",
    "\n",
    "# Creating the sorted_ev_centrality value\n",
    "sorted_ev_centrality = sorted(\n",
    "    ev_centrality.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# Finding the summary stats on eigenvector centrality\n",
    "eg_list = list(ev_centrality.values())\n",
    "\n",
    "egmean = np.mean(eg_list)\n",
    "egmin = np.min(eg_list)\n",
    "egmax = np.max(eg_list)\n",
    "egstd = np.std(eg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same format as Eigenvector and Betweenness so for comments, see there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_centrality = nx.degree_centrality(network)\n",
    "nx.set_node_attributes(network, deg_centrality, 'degree centrality')\n",
    "sorted_deg_centrality = sorted(\n",
    "    deg_centrality.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "deg_list = list(deg_centrality.values())\n",
    "\n",
    "degcenmean = np.mean(deg_list)\n",
    "degcenmin = np.min(deg_list)\n",
    "degcenmax = np.max(deg_list)\n",
    "degcenstd = np.std(deg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closeness Centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_centrality = nx.closeness_centrality(network)\n",
    "nx.set_node_attributes(network, close_centrality, 'closeness centrality')\n",
    "\n",
    "sorted_close_centrality = sorted(\n",
    "    close_centrality.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "close_list = list(close_centrality.values())\n",
    "\n",
    "closemean = np.mean(close_list)\n",
    "closemin = np.min(close_list)\n",
    "closemax = np.max(close_list)\n",
    "closestd = np.std(close_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cen_df = pd.DataFrame({'degree': deg_centrality, 'ev_centrality': ev_centrality,\n",
    "                   'betweenness_centrality': bet_centrality, 'close_centrality': close_centrality}\n",
    "                )\n",
    "# --+ correlation matrix\n",
    "cen_df.corr()\n",
    "# --+ scatter plot matrix\n",
    "sns.pairplot(cen_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Summary Table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n8/x7x_0smj0lj4_vlzr9zxvmwh0000gn/T/ipykernel_3004/531228237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# I'm sure there is a much better way to create this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummarytable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msummarytable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'closeness'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclosemean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosemin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosemax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosestd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msummarytable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eigenvector'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0megmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0megmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0megmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0megstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# I'm sure there is a much better way to create this\n",
    "summarytable = pd.DataFrame()\n",
    "\n",
    "summarytable['closeness'] = (closemean, closemin, closemax, closestd)\n",
    "summarytable['eigenvector'] = (egmean, egmin, egmax, egstd)\n",
    "summarytable['betweenness'] = (betmean, betmin, betmax, betstd)\n",
    "summarytable['degree centrality'] = (degcenmean, degcenmin, degcenmax, degcenstd)\n",
    "summarytable['degree'] = (degreemean, degreemin, degreemax, degreestd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I created this as a reference for all the variables I made\n",
    "\n",
    "# closemean\n",
    "# closemin\n",
    "# closemax\n",
    "# closestd\n",
    "\n",
    "\n",
    "# egmean\n",
    "# egmin\n",
    "# egmax\n",
    "# egstd\n",
    "\n",
    "# betmean\n",
    "# betmin\n",
    "# betmax \n",
    "# betstd\n",
    "\n",
    "# degcenmean \n",
    "# degcenmin \n",
    "# degcenmax \n",
    "# degcenstd \n",
    "\n",
    "\n",
    "# degreemean\n",
    "# degreemin\n",
    "# degreemax\n",
    "# degreestd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Girvan-Newman Method (aka the one that took a long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_solutions = girvan_newman(network)\n",
    "\n",
    "k = 20   # this is a value that we are supposed to adjust. Default is 10\n",
    "\n",
    "# register modularit scores\n",
    "modularity_scores = dict()     #change to user_modularity for easier ref\n",
    "\n",
    "# iterate over solutions AKA the one that takes a long time\n",
    "for community in itertools.islice(user_solutions, k):\n",
    "    solution = list(sorted(c) for c in community)\n",
    "    score = modularity(G_userSpace, solution)\n",
    "    modularity_scores[len(solution)] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Louvain Method (aka the one that took a few seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the Louvain class\n",
    "\n",
    "# Note that this ONLY works with undirected network. May need to artificially generate\n",
    "# an undirected copy of our graph\n",
    "partition = community_louvain.best_partition(network)\n",
    "\n",
    "# Grouping nodes via communities\n",
    "max_k_w = []\n",
    "for com in set(partition.values()):\n",
    "    list_nodes = [nodes for nodes in partition.keys()\n",
    "                  if partition[nodes] == com]\n",
    "    max_k_w = max_k_w + [list_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to check the number of communities\n",
    "len(max_k_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see how many communities are in community 1\n",
    "len(max_k_w[0])    # how many nodes are in that community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating the communtiy of binance\n",
    "\n",
    "binance_community = partition['0x28c6c06298d514db089934071355e5743bf21d60'] \n",
    "len(max_k_w[binance_community])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others\n",
    "\n",
    "# some other functions\n",
    "num_community = len(max_k_w)\n",
    "        self.num_community = num_community\n",
    "        print('Network has' + num_community + ' number of communities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "505bdda8a96b3799859aebe6232df39f95ae3c910af314e38caef1cf08e11b81"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('smm638': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
